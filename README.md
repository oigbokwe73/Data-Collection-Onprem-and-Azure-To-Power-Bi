# Data-Collection-Onprem-and-Azure-To-Power-Bi
Data Collection of Logs from Onprem and Azure for reporting through Power BI
import pandas as pd

# Define the table structure
data = {
    "Azure Resource": [
        "Azure Function App", 
        "Azure Storage", 
        "Azure Blob Containers", 
        "Azure Storage File Share", 
        "Virtual Network", 
        "User Defined Route Table", 
        "Azure EventHub", 
        "Azure SQL", 
        "Power BI"
    ],
    "Detailed Case": [
        "Used to trigger and execute custom code for processing logs, transforming data, and publishing to Event Hub or Azure SQL for further analysis.", 
        "Central storage solution for keeping log data, diagnostic outputs, and intermediate data processed by other Azure services.",
        "Blob Containers are used to store large volumes of logs or backup data that can be queried or processed for reporting or troubleshooting.", 
        "File Share stores persistent logs generated by services like MoveIt SFTP or application diagnostics, accessible by multiple services.",
        "Enables secure communication between Azure services and helps segment network traffic, ensuring resources are only accessible to authorized services or users.",
        "Custom routing for virtual networks to direct log data traffic between resources or to restrict traffic flows for better security and network control.",
        "Ingests large volumes of logs in real-time, supporting scalability for applications that generate significant amounts of diagnostic or transactional logs.",
        "Structured storage of log data, allowing complex queries and data analysis, serving as a reliable backend for reporting tools like Power BI.",
        "Power BI connects to data sources like Azure SQL to generate dashboards, visualizing log data trends, diagnostics, and business KPIs."
    ],
    "Collection Logs for Power BI": [
        "Collects and transforms logs before sending structured data to EventHub or SQL for Power BI.", 
        "Stores raw log data for later processing and querying.", 
        "Stores processed or raw logs which can be linked to Power BI for reporting.", 
        "Persistent logs from SFTP, available for querying through Azure Function or direct integration with Power BI.", 
        "Provides secure routing and access control for log collection and report generation in Power BI.",
        "Routes logs to appropriate storage or processing resources, ensuring smooth data flow for Power BI.",
        "Provides real-time log data ingestion, supporting near real-time reporting in Power BI dashboards.",
        "Stores structured log data, enabling Power BI to directly query and visualize logs.", 
        "Generates reports and dashboards by querying structured data from Azure SQL or Blob storage."
    ],
    "Category": [
        "Compute", 
        "Storage", 
        "Storage", 
        "Storage", 
        "Networking", 
        "Networking", 
        "Messaging", 
        "Database", 
        "Analytics"
    ],
    "Azure DevOps(CI/CD) with Terraform": [
        "Deployed using Azure DevOps Pipelines to automate function deployment and scaling.", 
        "Provisioned via Terraform in CI/CD pipelines to automate storage account creation and configuration.", 
        "Created and managed with Terraform to dynamically store and process logs.", 
        "Configured via Terraform to provide file share access for SFTP servers or applications in automated deployment processes.",
        "Provisioned via Terraform scripts to configure secure network connectivity for CI/CD processes.", 
        "Deployed with Terraform to ensure routing rules are set in line with organizational policies.", 
        "Configured using Terraform to automate setup of event hub for log ingestion.", 
        "Automated using Terraform to provision SQL databases for log storage and querying.", 
        "Integrated with Azure DevOps for continuous reporting updates using data connectors."
    ],
    "Diagnostics Logs": [
        "Processes diagnostic logs from various services to filter and store in Event Hub or Azure SQL.", 
        "Stores raw or processed diagnostic logs for further analysis.", 
        "Stores diagnostic logs from applications, making them accessible for processing.", 
        "Stores diagnostics logs from applications or infrastructure components for later querying.", 
        "Manages diagnostic logs traffic between resources securely.", 
        "Directs diagnostic logs traffic to appropriate resources for further processing.", 
        "Ingests diagnostic logs from various sources for real-time processing.", 
        "Stores diagnostic logs for easy access and analysis.", 
        "Visualizes diagnostic logs trends and patterns in real-time or near-real-time."
    ],
    "Development": [
        "Used in dev environments to quickly process and debug logs.", 
        "Holds test log data for development purposes.", 
        "Stores development logs and application output for analysis.", 
        "Stores persistent development logs for testing and debugging purposes.", 
        "Used to isolate development traffic from production environments.", 
        "Configures routing specific to development environments.", 
        "Ingests dev logs for real-time testing and validation.", 
        "Stores structured dev logs for further analysis.", 
        "Connects to dev databases and datasets to create test reports."
    ],
    "Maintenance": [
        "Maintained by updating function code and monitoring for performance.", 
        "Monitored for capacity and performance to ensure data storage limits are not exceeded.", 
        "Checked for lifecycle management to automatically delete old logs.", 
        "Monitored to ensure logs are accessible and storage is properly allocated.", 
        "Monitored for secure network configuration and throughput.", 
        "Reviewed to ensure routes remain valid and secure over time.", 
        "Maintained to handle high-volume log ingestion and scalability.", 
        "Monitored for performance and query efficiency.", 
        "Monitored to ensure dashboards are up-to-date and connected data sources are active."
    ]
}

# Create a DataFrame
df = pd.DataFrame(data)

# Display the DataFrame to the user
import ace_tools as tools; tools.display_dataframe_to_user(name="Azure Resource Case Study", dataframe=df)
